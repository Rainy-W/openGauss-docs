# 数据校验<a name="ZH-CN_TOPIC_0000001398052181"></a>

## 功能介绍<a name="section1480816250617"></a>

数据校验工具 gs_datacheck，分为check服务和extract服务。check服务用于数据校验，extract服务用于数据抽取和规整。

## 原理介绍<a name="section1480816250617"></a>

全量校验：

在全量数据迁移完成后，由extract服务对MySQL源端和openGauss目标端数据通过JDBC方式进行数据抽取然后规整计算，并将计算后的中间数据推送到kafka中。最后由check服务提取kafka中的中间数据，构建默克尔树，通过默克尔树比对实现表数据校验且输出校验结果。

增量校验：

由debezium服务侦听源端MySQL数据库的增量数据，到指定topic。再由源端extract服务处理该topic增量数据，触发check增量校验。

## 环境准备<a name="section425318254413"></a>

-  ARM+openEuler 20.03 或 X86+CentOS 5.7 
- JDK ： JDK11+
- MYSQL：要求5.7+版本
- openGauss：openGauss3.0.0+

##  操作步骤

全量校验 gs_datacheck 依赖MySQL一键式迁移工具gs_rep_portal，可实现全量迁移的安装、启动、停止、卸载整个过程。

- 下载gs_rep_portal

  ```
  wget https://opengauss.obs.cn-south-1.myhuaweicloud.com/tools/portal/PortalControl-5.0.0.tar.gz
  ```

  解压，并进入portal对应目录

  ```
  tar -zxvf PortalControl-5.0.0.tar.gz
  cd portal
  ```

- 修改gs_rep_portal配置文件

  配置文件位于config目录内，数据校验相关的配置文件主要包含如下三个，相关参数含义简要说明如下：

- （1）application.yml

  ```
  校验服务配置 修改application.yml文件
  	server.port 为校验服务web端口，默认可不修改
  	logging.config  设置校验服务日志路径为config/log4j2.xml文件绝对路径
  	bootstrap-servers 为kafka工作地址，默认安装可不修改
  	data.check.data-path 校验结果输出地址，默认配置可不修改
  	data.check.source-uri 源端服务请求地址，默认配置可不修改
  	data.check.sink-uri 目标端服务请求地址，默认配置可不修改
  	data.check.core-pool-size 并发线程数设置，根据当前环境配置，可不修改，默认10，设置为0则系统自动分配
  	data.check.max-retry-times 心跳等最大尝试次数，默认1000
  	data.check.retry-interval-times 心跳、进度等最大间隔时间单位毫秒 10000
      data.check.auto-delete-topic: 配置是否自动删除Topic，0不删除，1校验全部完成后删除，2表校验完
      成后删除，默认值为2
      data.check.increment-max-diff-count: 1000 增量校验表最大处理差异数，超过则暂停校验，差异降低则重新自动开启增量校验
  	rules.enable 开启规则校验
  	rules.tables 添加表过滤规则
  	rules.row 添加行过滤规则
  	rules.column 添加列过滤规则
  ```

  （2）application-source.yml

  ```
  源端服务配置 修改application-source.yml文件
  	server.port 为源端抽取服务web端口，默认可不修改
  	logging.config 设置校验服务日志路径为config/log4j2source.xml文件绝对路径
  	spring.check.server-uri 校验服务请求地址，默认配置可不修改
  	spring.extract.schema 当前校验数据schema，mysql 数据库名称
  	spring.extract.core-pool-size 并发线程数设置，根据当前环境配置，可不修改，默认10，设置为0则系统自动分配
  	spring.extract.query-dop 表JDBC并行处理度，当表数据量超过百万时自动生效，默认为8，最大64
  	spring.extract.max-retry-times 心跳等最大尝试次数，默认1000
  	spring.extract.retry-interval-times 心跳、进度等最大间隔时间单位毫秒 10000
  	bootstrap-servers 为kafka工作地址，默认安装可不修改
  	
  	数据源配置
  	工具默认采用druid数据源，用户可以自定义配置连接池参数,可根据当前校验数据库任务数量（表数量）进行调整 
  	initialSize: 5 默认初始连接大小
  	minIdle: 10 默认最小连接池数量
  	maxActive: 20 默认激活数据库连接数量，maxActive 大于query-dop 一般为query-dop 的2-3倍       
  
  ```

  （3）application-sink.yml

  ```
  目标端服务配置 修改application-sink.yml文件
  	server.port 为目标端抽取服务web端口，默认可不修改
  	logging.config 设置校验服务日志路径为config/log4j2sink.xml文件绝对路径
  	spring.check.server-uri 校验服务请求地址，默认配置可不修改
  	spring.extract.schema 当前校验数据schema，opengauss schema名称
  	spring.extract.core-pool-size 并发线程数设置，根据当前环境配置，可不修改，默认10，设置为0则系统自动分配
  	spring.extract.query-dop 表JDBC并行处理度，当表数据量超过百万时自动生效，默认为8，最大64
  	spring.extract.max-retry-times 心跳等最大尝试次数，默认1000
  	spring.extract.retry-interval-times 心跳、进度等最大间隔时间单位毫秒 10000
  	bootstrap-servers 为kafka工作地址，默认安装可不修改
  	
  	数据源配置
  	工具默认采用druid数据源，用户可以自定义配置连接池参数，可根据当前校验数据库任务数量（表数量）进行调整
  	initialSize: 5 默认初始连接大小
  	minIdle: 10 默认最小连接池数量
      maxActive: 20 默认激活数据库连接数量， maxActive 大于query-dop 一般为query-dop 的2-3倍 
  ```

- 安装

  ```
  # 全量校验
  sh gs_datacheck.sh install full workspace.id  
  # 增量校验
  sh gs_datacheck.sh install incremental workspace.id
  ```

  其中workspace.id表示迁移任务id，取值为一个整数，不同的id区分不同的校验任务，不同校验任务可并行启动。full 表示全量校验 ，incremental 表示增量校验

- 启动

  ```
  # 全量校验
  sh gs_datacheck.sh start full workspace.id  
  # 增量校验
  sh gs_datacheck.sh start incremental workspace.id
  ```

- 停止

  ```
  # 全量校验
  sh gs_datacheck.sh stop full workspace.id  
  # 增量校验
  sh gs_datacheck.sh stop incremental workspace.id
  ```

- 卸载

  ```
  # 全量校验
  sh gs_datacheck.sh uninstall full workspace.id  
  # 增量校验
  sh gs_datacheck.sh uninstall incremental workspace.id
  ```


## 注意事项<a name="section17604122817181"></a>

	-  JDK版本要求JDK11+
	-  当前版本仅支持对源端MySQL，目标端openGauss数据校验
	-  当前版本仅支持数据校验，不支持表对象校验
	-  MYSQL需要5.7+版本
	-  当前版本不支持地理位置几何图形数据校验
	-  校验工具当前不支持校验中断(网络故障、kill进程等)自动恢复。
	-  数据校验行级过滤规则配置，只支持[offset,count]指定范围内抽取，不支持排除[offset,count]范围之内的数据过滤。
	-  行过滤规则抽取中间范围内数据（例如：[10,100]），如果源端在该范围之前的数据[0,10]发生删除操作，则会导致该表在指定范围内数据发生偏移，从而导致数据校验结果产生差异。此时需要扩大前置下标范围,以及增加相应的抽取数量。即[3,107]。
	-  当对主键的update语句没有通过增量迁移同步到目的端 或 主键同步发生错误的时候，进行数据校验，源端update后的新数据和目标端的旧数据是两条独立的数据，对校验差异进行处理时，会生成两条语句，即对旧数据进行删除，对新数据做插入。此场景会将一条主键update语句拆分为两条语句（insert+delete）来执行，且分解到两个事务中执行，无法保证原子性。
	-  增量校验不支持表级规则
	-  增量校验不支持行级规则
	-  增量校验目前只支持数据增删改校验，暂时不支持表结构（对象）校验（包括多表少表）

