# 数据校验<a name="ZH-CN_TOPIC_0000001398052181"></a>

数据校验项目openGauss-tools-datachecker-performance，分为check服务和extract服务。check服务用于数据校验，extract服务用于数据抽取和规整。

## 原理介绍<a name="section1480816250617"></a>

全量校验：

在全量数据迁移完成后，由extract服务对MySQL源端和openGauss目标端数据抽取然后规整，并将数据推送到kafka中。最后由check服务提取kafka中的数据，并进行校验且输出校验结果。

增量校验：

由debezium服务侦听源端MySQL数据库的增量数据，到指定topic。再由源端extract服务处理该topic增量数据，触发check增量校验。

## 环境准备<a name="section425318254413"></a>

- 服务器数量：3台鲲鹏920服务器（2台用于数据库服务器，1台用于校验服务端，kafka服务）。

- 服务器硬件规格：

  - Memory：大于512GB。
  - CPU: Kunpeng-920 2600MHZ 128核
  - Free Disk：4块NVME硬盘，每块容量大于1TB。
  - 网卡：Hi1822千兆网卡，光纤互连。

- 操作系统要求：openEuler-20.03-LTS（aarch64 架构）

- JDK ： JDK11+

- MYSQL：要求5.7+版本

- openGauss：openGauss3.0.0+

  ```
  查看openGauss synchronize_seqscans参数，并关闭synchronize_seqscans
  show synchronize_seqscans;
  gs_guc set -D datadir -c "synchronize_seqscans=off";
  或者
  gs_guc set -N all -I all -c "synchronize_seqscans=off"
  查看openGauss query_dop 参数，开启并行查询 query_dop 参数，设置并行度为8，可以根据机器情况自行配置，最大64。
  gs_guc set -D datadir -c "query_dop=8";
  or
  gs_guc set -N all -I all -c "query_dop=8";
  ```

  

## 工具源码安装<a name="section1912981915448"></a>

-   安装软件依赖：

    jdk11 、git 、maven 、kafka、debezium（增量校验-源端connect）

-   安装及操作步骤：
    1.  通过git命令下载源码。

        ```
        git clone https://gitee.com/opengauss/openGauss-tools-datachecker-performance.git
        ```

    2.  通过maven命令构建check 和 extract jar包

        ```
        mvn clean package -Dmaven.test.skip=true
        ```

    3.  复制Jar包和config目录到指定部署目录下。
    4.  配置相关配置config。
        -   校验端配置文件application.yml：
            ```
            校验服务配置 修改application.yml文件
            	server.port 为校验服务web端口，默认可不修改
            	logging.config  设置校验服务日志路径为config/log4j2.xml文件绝对路径
            	bootstrap-servers 为kafka工作地址，默认安装可不修改
            	data.check.data-path 校验结果输出地址，默认配置可不修改
            	data.check.source-uri 源端服务请求地址，默认配置可不修改
            	data.check.sink-uri 目标端服务请求地址，默认配置可不修改
            	data.check.core-pool-size 并发线程数设置，根据当前环境配置，可不修改，默认10，设置为0则系统自动分配
            	data.check.max-retry-times 心跳等最大尝试次数，默认1000
            	data.check.retry-interval-times 心跳、进度等最大间隔时间单位毫秒 10000
                data.check.auto-delete-topic: 配置是否自动删除Topic，0不删除，1校验全部完成后删除，2表校验完成后删除，默认值为2
            	rules.enable 开启规则校验
            	rules.tables 添加表过滤规则
            	rules.row 添加行过滤规则
		    	rules.column 添加列过滤规则
            ```
        -   抽取端-源端配置 application-source.yml：
            ```
        	源端服务配置 修改application-source.yml文件
            	server.port 为源端抽取服务web端口，默认可不修改
            	logging.config 设置校验服务日志路径为config/log4j2source.xml文件绝对路径
            	spring.check.server-uri 校验服务请求地址，默认配置可不修改
            	spring.extract.schema 当前校验数据schema，mysql 数据库名称
            	spring.extract.core-pool-size 并发线程数设置，根据当前环境配置，可不修改，默认10，设置为0则系统自动分配
            	spring.extract.max-retry-times 心跳等最大尝试次数，默认1000
            	spring.extract.retry-interval-times 心跳、进度等最大间隔时间单位毫秒 10000
            	bootstrap-servers 为kafka工作地址，默认安装可不修改
            	
            	数据源配置
            	工具默认采用druid数据源，用户可以自定义配置连接池参数,可根据当前校验数据库任务数量（表数量）进行调整 
            	initialSize: 5 默认初始连接大小
            	minIdle: 10 默认最小连接池数量
            	maxActive: 20 默认激活数据库连接数量         
            ```
        -   抽取端-目标端配置 application-sink.yml：       
            ```
            目标端服务配置 修改application-sink.yml文件
            	server.port 为目标端抽取服务web端口，默认可不修改
            	logging.config 设置校验服务日志路径为config/log4j2sink.xml文件绝对路径
            	spring.check.server-uri 校验服务请求地址，默认配置可不修改
		    	spring.extract.schema 当前校验数据schema，opengauss schema名称
            	spring.extract.core-pool-size 并发线程数设置，根据当前环境配置，可不修改，默认10，设置为0则系统自动分配
    	    	spring.extract.max-retry-times 心跳等最大尝试次数，默认1000
            	spring.extract.retry-interval-times 心跳、进度等最大间隔时间单位毫秒 10000
            	bootstrap-servers 为kafka工作地址，默认安装可不修改
            	
        		数据源配置
            	工具默认采用druid数据源，用户可以自定义配置连接池参数，可根据当前校验数据库任务数量（表数量）进行调整
        		initialSize: 5 默认初始连接大小
            	minIdle: 10 默认最小连接池数量
                maxActive: 20 默认激活数据库连接数量
            ```
        
    5.  启动服务。
        -   zookeeper启动。   
            ```
            cd /data/kafka/confluent-7.2.0
            bin/zookeeper-server-start -daemon etc/kafka/zookeeper.properties
            ```
        
        -   kafka启动。   
            ```
            bin/kafka-server-start -daemon etc/kafka/server.properties
            ```
            
        -   connect debezium连接器启动（增量校验要求启动），my-mysql-connect.properties为配置debezium连接器配置文件。  
        
            ```
            bin/connect-standalone -daemon etc/kafka/connect-standalone.properties etc/kafka/my-mysql-connect.properties
            ```
	
    
    
    6.  启动抽取校验服务。   
        ```
	    sh extract-endpoints.sh stat|restart|stop
	sh check-endpoint.sh stat|restart|stop
	    ```
	
	7.  服务启动后，自动开启全量校验流程。
	
	8.  启动增量校验（通过修改源端配置文件启动）。
	    ```
		debezium-enable: true
	    并配置其他debezium-相关配置，启动服务即可开启增量校验服务。
	    ```
	
	
	**详细使用说明见 《[校验工具使用指导](https://gitee.com/opengauss/openGauss-tools-datachecker-performance/blob/master/校验工具使用指导.md)》**

## 工具二进制安装<a name="section33621845504"></a>

	通过如下链接下载压缩包并解压，并配置相关配置文件，通过shell脚本即可启动服务。具体配置信息和操作步骤可参考源码安装部分。
	https://opengauss.obs.cn-south-1.myhuaweicloud.com/latest/tools/openGauss-datachecker-performance-5.0.0.tar.gz
	tar -zxvf openGauss-datachecker-performance-5.0.0.tar.gz
	
	解压目录包含：
	-  datachecker-check-0.0.1.jar：校验服务Jar
	-  datachecker-extract-0.0.1.jar：抽取服务Jar
	-  check-endpoint.sh：校验服务启动脚本
	-  extract-endpoints.sh：源端和目的端数据抽取服务启动脚本
	
	config目录包含：
	-  application.yml：校验端配置文件
	-  application-source.yml：源端配置文件
	-  application-sink.yml：目标端配置文件
	-  log4j2.xml
	-  log4j2sink.xml
	-  log4j2source.xml

## 卸载工具<a name="section1174761844514"></a>

	删除对应JAR包，及相关配置文件即可。

## 注意事项<a name="section17604122817181"></a>

	-  JDK版本要求JDK11+
	-  当前版本仅支持对源端MySQL，目标端openGauss数据校验
	-  当前版本仅支持数据校验，不支持表对象校验
	-  MYSQL需要5.7+版本
	-  当前版本不支持地理位置几何图形数据校验
	-  校验工具当前不支持校验中断(网络故障、kill进程等)自动恢复。
	-  数据校验行级过滤规则配置，只支持[offset,count]指定范围内抽取，不支持排除[offset,count]范围之内的数据过滤。
	-  行过滤规则抽取中间范围内数据（例如：[10,100]），如果源端在该范围之前的数据[0,10]发生删除操作，则会导致该表在指定范围内数据发生偏移，从而导致数据校验结果产生差异。此时需要扩大前置下标范围,以及增加相应的抽取数量。即[3,107]。
	-  当对主键的update语句没有通过增量迁移同步到目的端 或 主键同步发生错误的时候，进行数据校验，源端update后的新数据和目标端的旧数据是两条独立的数据，对校验差异进行处理时，会生成两条语句，即对旧数据进行删除，对新数据做插入。此场景会将一条主键update语句拆分为两条语句（insert+delete）来执行，且分解到两个事务中执行，无法保证原子性。
	-  增量校验不支持表级规则
	-  增量校验不支持行级规则
	-  增量校验目前只支持数据增删改校验，暂时不支持表结构（对象）校验（包括多表少表）

