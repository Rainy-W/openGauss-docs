# Query Native Compilation \(JIT\)<a name="EN-US_TOPIC_0257867445"></a>

## JIT Compilation Comparison: openGauss Regular vs. MOT tables<a name="en-us_topic_0257720403_section27441261255"></a>

Current GaussDB contains two main forms of JIT / CodeGen query optimizations for its disk-based tables: \(1\) Accelerating expression evaluation \(such as in WHERE clauses, target lists, aggregates and projections\), and \(2\) Inlining small function invocations. These optimizations are partial \(in the sense they do not optimize the entire interpreted operator tree or replace it altogether\), and are targeted mostly at CPU-bound complex queries, typically seen in OLAP use cases. The execution of queries is performed in a pull-model \(Volcano-style processing\) using an interpreted operator tree. When activated, the compilation is performed at each query execution. At the moment, caching of the generated LLVM code and its reuse across sessions and queries is not present.

In contrast, MOT JIT optimization provides an LLVM code for entire queries that qualify JIT optimization by MOT. The result code is used for direct execution over MOT tables, while the interpreted operator model is abandoned completely. The result is a practically hand-written LLVM code generated for specific and entire query execution. Another significant conceptual difference is that MOT LLVM code is generated only for prepared queries, during the PREPARE phase of the query, rather than at query execution. This is especially important for OLTP scenarios due to the rather short runtime of OLTP queries, which cannot allow for code generation and relatively long query compilation time to be performed during each query execution. At last, in PostgreSQL the activation of PREPARE implies reuse the result plan across execution with different parameters in the same session. Similarly, the MOT JIT applies caching policy for its LLVM code results, and extends it for reuse across different sessions. Thus a single query may be compiled just once and its LLVM code reused across many sessions, which again favorable for OLTP scenarios.

